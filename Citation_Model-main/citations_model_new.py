# -*- coding: utf-8 -*-
"""citations_model_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NSFrpn2sTyVN-9ceKvLd4hfEeJ3w7lpC
"""

from google.colab import drive
drive.mount('/content/drive')

BASE_DIR = '/content/drive/MyDrive/data_nlp'
PRED_PATH = os.path.join(BASE_DIR, 'predictions.csv')

"""Conncect with google drive and Importing the paths

install some packages
"""

!pip install --quiet spacy networkx scikit-learn joblib
!python -m spacy download en_core_web_sm
!pip install catboost

"""Importing the libraries"""

import os
import numpy as np
import pandas as pd
import networkx as nx
import re
import joblib
import torch

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from xgboost import XGBClassifier
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from catboost import CatBoostClassifier
from transformers import RobertaTokenizer, RobertaModel

"""Load the dataset"""

abstracts = pd.read_csv(f'{BASE_DIR}/abstracts.txt', sep=r'\|\-\-\|', engine='python', names=['id', 'text'], header=None)
authors   = pd.read_csv(f'{BASE_DIR}/authors.txt',   sep=r'\|\-\-\|', engine='python', names=['id', 'auth'], header=None)
edgelist  = pd.read_csv(f'{BASE_DIR}/edgelist.txt', names=['src', 'tgt'], header=None)

meta = abstracts.merge(authors, on='id').reset_index(drop=True)
all_ids = meta['id'].values
id2idx = {}
for i, pid in enumerate(all_ids):
    id2idx[pid] = i

"""Graph creation for identifying edges between papers"""

all_ids = meta['id'].values
id2idx = {pid:i for i,pid in enumerate(all_ids)}
G = nx.Graph()
G.add_edges_from(edgelist.values)
neighbors = {n:set(G.neighbors(n)) for n in G.nodes()}
degrees = {nid:G.degree(nid) for nid in all_ids}

"""precompute rage rank and clustering"""

pagerank = nx.pagerank(G)
clustering = nx.clustering(G)
pr_array = np.array([pagerank.get(pid, 0) for pid in all_ids], dtype=float)
clust_array = np.array([clustering.get(pid, 0) for pid in all_ids], dtype=float)

"""Text preprocessing"""

raw_texts = meta['text'].fillna('').tolist()
proc_texts = [" ".join(re.findall(r"\b[a-zA-Z]{3,}\b", t.lower())) for t in raw_texts]

"""tfidf"""

tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
X_tfidf = tfidf.fit_transform(proc_texts).toarray()
# Precompute top-k token sets for Jaccard
k = 50
topk_tokens = []
for vec in X_tfidf:
    top_idx = np.argsort(vec)[-k:]
    topk_tokens.append(set(top_idx))

"""Embeddings with SentenceTransformer()"""

model_embed = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
X_doc_vecs = model_embed.encode(proc_texts, show_progress_bar=True, batch_size=64, convert_to_numpy=True)

"""roBERTa tokenization"""

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model_roberta = RobertaModel.from_pretrained('roberta-base').to('cuda')
model_roberta.eval()

# tokenize and encoding in only one batch

batch_size = 16  # ή ακόμα και 8
embeddings = []
for i in range(0, len(proc_texts), batch_size):
    batch = proc_texts[i:i+batch_size]
    enc = tokenizer(batch, padding='longest', truncation=True, max_length=128, return_tensors='pt').to('cuda')
    with torch.no_grad():
        out = model_roberta(**enc)
    embeddings.append(out.last_hidden_state[:, 0, :].cpu().numpy())
    torch.cuda.empty_cache()

X_doc_vecs_roberta = np.vstack(embeddings)

"""Author sets for Jaccard Similarity"""

auth_sets = meta['auth'].fillna('').str.split(',').map(lambda L: {a.strip() for a in L}).tolist()

"""Function for JaccardSimilarity"""

def jaccard(a, b):
    return float(len(a & b)) / len(a | b) if (a | b) else 0.0

"""Negative/Positive pair creation"""

pos = edgelist.values.tolist()
rng = np.random.default_rng(42)
neg = []

while len(neg) < len(pos):
    u, v = rng.choice(all_ids, 2, replace=False)
    if not G.has_edge(u, v):
        neg.append([u, v])

pairs = np.array(pos + neg)
labels = np.array([1] * len(pos) + [0] * len(neg))

"""Feature Extraction"""

features = []
for u, v in pairs:
    ui, vi = id2idx[u], id2idx[v]
    # semantic embeddings
    vec_u, vec_v = X_doc_vecs[ui], X_doc_vecs[vi]
    sim = np.dot(vec_u, vec_v)
    dist = np.linalg.norm(vec_u - vec_v)

    #roberta embeddings
    vec_u1, vec_v1 = X_doc_vecs_roberta[ui], X_doc_vecs_roberta[vi]
    sim_roberta = np.dot(vec_u1, vec_v1)
    cos_roberta = sim_roberta / (
        (np.linalg.norm(vec_u1) * np.linalg.norm(vec_v1)) + 1e-8
    )
    dist_roberta = np.linalg.norm(vec_u1 - vec_v1)

    # TF-IDF features
    tfidf_sim = np.dot(X_tfidf[ui], X_tfidf[vi]) / (
        np.linalg.norm(X_tfidf[ui]) * np.linalg.norm(X_tfidf[vi]) + 1e-8)
    jacc_tfidf = jaccard(topk_tokens[ui], topk_tokens[vi])

    # authors
    auth_sim = jaccard(auth_sets[ui], auth_sets[vi])

    # graph features
    deg_diff = abs(degrees[ui] - degrees[vi])
    cn = len(neighbors[u] & neighbors[v])
    aa_score = sum(1 / np.log(degrees[id2idx[w]]) for w in (neighbors[u] & neighbors[v]) if degrees[id2idx[w]] > 1)
    pr_diff = abs(pr_array[ui] - pr_array[vi])
    clust_diff = abs(clust_array[ui] - clust_array[vi])

    # token lengths
    tok_diff = abs(len(proc_texts[ui].split()) - len(proc_texts[vi].split()))
    features.append([sim, dist, sim_roberta, cos_roberta, dist_roberta, tfidf_sim, jacc_tfidf, auth_sim,
                     deg_diff, cn, aa_score, pr_diff, clust_diff, tok_diff])
X = np.array(features)

"""scaling the features"""

scaler = StandardScaler()
X = scaler.fit_transform(X)

"""train test split"""

X_tr, X_val, y_tr, y_val = train_test_split(X, labels, test_size=0.1, stratify=labels, random_state=42)

"""XGBoostmodel training"""

xg_model = XGBClassifier(
    objective='binary:logistic', n_jobs=-1, random_state=42,
    max_depth=7, learning_rate=0.05, n_estimators=400,
    subsample=0.8, colsample_bytree=0.8, scale_pos_weight=1
)
xg_model.fit(X_tr, y_tr)

"""catboost training"""

# catboost_params = {
#     'iterations': 300,
#     'learning_rate': 0.01,
#     'eval_metric': 'Accuracy',
#     'task_type': 'GPU',
#     'early_stopping_rounds': 20,
#     'verbose': 50
# }

cat_model = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.05,
    depth=8,
    loss_function='Logloss',
    eval_metric='Accuracy',
    random_seed=42,
    od_type='Iter',
    od_wait=50,
    task_type='GPU',
    verbose=100
)
cat_model.fit(X_tr, y_tr)

"""svm"""

# from sklearn.svm import SVC

# svm_model = SVC(kernel='linear', probability=True, random_state=42)
# svm_model.fit(X_tr, y_tr)

"""evaluation"""

val_preds_xg = xg_model.predict_proba(X_val)[:, 1]
val_preds_cat = cat_model.predict_proba(X_val)[:, 1]
final_probs_AUC = (xg_preds + cat_preds) / 2
print("Validation AUC:", roc_auc_score(y_val, final_probs_AUC))
# val_preds_svm = svm_model.predict_proba(X_val)[:, 1]
# print("validation AUC:", roc_auc_score(y_val, val_preds_svm))

"""test prediction"""

pairs_test = pd.read_csv(os.path.join(BASE_DIR, 'test.txt'), names=['src', 'tgt']).values
# extract test features using same logic
features_test = []
for u, v in pairs_test:

    #semantic embeddings
    ui, vi = id2idx[u], id2idx[v]
    vec_u, vec_v = X_doc_vecs[ui], X_doc_vecs[vi]
    sim = np.dot(vec_u, vec_v)
    dist = np.linalg.norm(vec_u - vec_v)

    #roberta embeddings
    vec_u1, vec_v1 = X_doc_vecs_roberta[ui], X_doc_vecs_roberta[vi]
    sim_roberta = np.dot(vec_u1, vec_v1)
    cos_roberta = sim_roberta / (
        (np.linalg.norm(vec_u1) * np.linalg.norm(vec_v1)) + 1e-8
    )
    dist_roberta = np.linalg.norm(vec_u1 - vec_v1)

    #tf-idf features
    tfidf_sim = np.dot(X_tfidf[ui], X_tfidf[vi]) / (
        np.linalg.norm(X_tfidf[ui]) * np.linalg.norm(X_tfidf[vi]) + 1e-8)
    jacc_tfidf = jaccard(topk_tokens[ui], topk_tokens[vi])

    #authors
    auth_sim = jaccard(auth_sets[ui], auth_sets[vi])

    # graph features
    deg_diff = abs(degrees[ui] - degrees[vi])
    cn = len(neighbors[u] & neighbors[v])
    aa_score = sum(1 / np.log(degrees[id2idx[w]]) for w in (neighbors[u] & neighbors[v]) if degrees[id2idx[w]] > 1)
    pr_diff = abs(pr_array[ui] - pr_array[vi])
    clust_diff = abs(clust_array[ui] - clust_array[vi])

    # token lengths
    tok_diff = abs(len(proc_texts[ui].split()) - len(proc_texts[vi].split()))
    features_test.append([sim, dist, sim_roberta, cos_roberta, dist_roberta, tfidf_sim, jacc_tfidf, auth_sim,
                          deg_diff, cn, aa_score, pr_diff, clust_diff, tok_diff])

# normalization
X_test = scaler.transform(np.array(features_test))

xg_preds = xg_model.predict_proba(X_test)[:, 1]
cat_preds = cat_model.predict_proba(X_test)[:, 1]

final_probs = (xg_preds + cat_preds) / 2
# final_probs = svm_model.predict_proba(X_test)[:, 1]

"""Model prediction and save .csv file"""

pd.DataFrame({'ID': range(len(final_probs)), 'label': final_probs}).to_csv(PRED_PATH, index=False, float_format='%.6f')